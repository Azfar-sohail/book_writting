"use strict";(globalThis.webpackChunkai_native_textbook_platform=globalThis.webpackChunkai_native_textbook_platform||[]).push([[335],{2827(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>g,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var a=t(4848),o=t(8453);const i={sidebar_position:18,title:"Chapter 18: Language to Action Planning"},s="Chapter 18: Language to Action Planning",r={id:"part2/language_to_action_planning",title:"Chapter 18: Language to Action Planning",description:"Introduction",source:"@site/docs/part2/18_language_to_action_planning.md",sourceDirName:"part2",slug:"/part2/language_to_action_planning",permalink:"/ai-native-textbook-platform/es/docs/part2/language_to_action_planning",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-humanoid-robots/tree/main/packages/create-docusaurus/templates/shared/docs/part2/18_language_to_action_planning.md",tags:[],version:"current",sidebarPosition:18,frontMatter:{sidebar_position:18,title:"Chapter 18: Language to Action Planning"},sidebar:"tutorialSidebar",previous:{title:"Chapter 17: Voice to Text with Whisper",permalink:"/ai-native-textbook-platform/es/docs/part2/voice_to_text_with_whisper"},next:{title:"Chapter 19: Autonomous Decision Making",permalink:"/ai-native-textbook-platform/es/docs/part2/autonomous_decision_making"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Grounded Language Understanding",id:"grounded-language-understanding",level:3},{value:"Task Planning and Decomposition",id:"task-planning-and-decomposition",level:3},{value:"Execution and Feedback",id:"execution-and-feedback",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Semantic Parser for Robot Commands",id:"example-1-semantic-parser-for-robot-commands",level:3},{value:"Example 2: Action Planning System",id:"example-2-action-planning-system",level:3},{value:"Diagram Placeholders",id:"diagram-placeholders",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"chapter-18-language-to-action-planning",children:"Chapter 18: Language to Action Planning"}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"Converting natural language commands into executable robot actions is a critical component of human-robot interaction. This chapter explores the techniques and architectures for translating human language into robot behavior through planning and execution systems."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the challenges of natural language understanding in robotics"}),"\n",(0,a.jsx)(e.li,{children:"Learn about semantic parsing and command interpretation"}),"\n",(0,a.jsx)(e.li,{children:"Explore action planning and task decomposition"}),"\n",(0,a.jsx)(e.li,{children:"Implement language-to-action systems for robotics"}),"\n",(0,a.jsx)(e.li,{children:"Recognize the importance of context and grounding"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,a.jsx)(e.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,a.jsx)(e.p,{children:"Processing human commands for robots:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Semantic Parsing"}),": Converting text to structured meaning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Named Entity Recognition"}),": Identifying objects, locations, and actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Intent Classification"}),": Understanding the purpose of commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context Resolution"}),": Handling pronouns and references"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Ambiguity Resolution"}),": Dealing with unclear or multiple interpretations"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"grounded-language-understanding",children:"Grounded Language Understanding"}),"\n",(0,a.jsx)(e.p,{children:"Connecting language to physical reality:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Spatial Grounding"}),": Understanding spatial relationships and locations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Object Grounding"}),": Connecting words to physical objects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Grounding"}),": Mapping language actions to robot capabilities"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Visual Context"}),": Using vision to disambiguate language"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"World Knowledge"}),": Incorporating prior knowledge about the environment"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"task-planning-and-decomposition",children:"Task Planning and Decomposition"}),"\n",(0,a.jsx)(e.p,{children:"Breaking down complex commands:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hierarchical Planning"}),": High-level to low-level action decomposition"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Symbolic Planning"}),": Using symbolic representations for planning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reactive Planning"}),": Adapting plans based on feedback"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Multi-Step Reasoning"}),": Planning sequences of actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Constraint Handling"}),": Managing preconditions and effects"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"execution-and-feedback",children:"Execution and Feedback"}),"\n",(0,a.jsx)(e.p,{children:"Executing planned actions and handling errors:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Execution"}),": Converting high-level plans to low-level commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitoring"}),": Tracking execution progress and detecting failures"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Recovery"}),": Handling execution failures and replanning"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Feedback"}),": Providing status updates to users"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Learning"}),": Improving from execution experience"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-semantic-parser-for-robot-commands",children:"Example 1: Semantic Parser for Robot Commands"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import re\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass RobotAction:\n    action_type: str  # \"navigate\", \"pick\", \"place\", \"open\", \"close\"\n    object_name: Optional[str] = None\n    location: Optional[str] = None\n    target: Optional[str] = None\n\nclass SemanticParser:\n    def __init__(self):\n        self.action_patterns = {\n            'navigate': [\n                r'go to (?:the )?(\\w+)',\n                r'move to (?:the )?(\\w+)',\n                r'go (?:to )?(?:the )?(\\w+)'\n            ],\n            'pick': [\n                r'pick up (?:the )?(\\w+)',\n                r'grasp (?:the )?(\\w+)',\n                r'get (?:the )?(\\w+)'\n            ],\n            'place': [\n                r'place (?:the )?(\\w+) (?:on|at) (?:the )?(\\w+)',\n                r'put (?:the )?(\\w+) (?:on|at) (?:the )?(\\w+)'\n            ]\n        }\n\n    def parse_command(self, command: str) -> List[RobotAction]:\n        command = command.lower().strip()\n        actions = []\n\n        for action_type, patterns in self.action_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, command)\n                if match:\n                    if action_type == 'place':\n                        # Handle two-argument commands\n                        obj, location = match.groups()\n                        actions.append(RobotAction(action_type, obj, location))\n                    elif len(match.groups()) == 1:\n                        obj = match.group(1)\n                        actions.append(RobotAction(action_type, obj))\n                    else:\n                        actions.append(RobotAction(action_type))\n\n        return actions\n\n# Example usage\nparser = SemanticParser()\ncommand = \"Pick up the red cup and place it on the table\"\nactions = parser.parse_command(command)\nprint(f\"Command: {command}\")\nprint(f\"Actions: {actions}\")\n"})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-action-planning-system",children:"Example 2: Action Planning System"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from enum import Enum\nfrom typing import Dict, Any\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import Pose\nfrom std_msgs.msg import String\n\nclass TaskStatus(Enum):\n    PENDING = "pending"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n\nclass LanguageToActionPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.current_task = None\n        self.task_status = TaskStatus.PENDING\n        self.semantic_parser = SemanticParser()\n\n        # ROS interfaces\n        self.nav_client = ActionClient(node, NavigateToPose, \'navigate_to_pose\')\n        self.manipulation_client = ActionClient(node, MoveToPose, \'move_to_pose\')\n        self.object_detector = node.create_subscription(\n            String, \'object_detection\', self.object_callback, 10)\n\n        self.known_objects = {}\n        self.known_locations = {}\n\n    def process_command(self, command: str):\n        """Process a natural language command and execute corresponding actions"""\n        # Parse the command\n        actions = self.semantic_parser.parse_command(command)\n\n        # Execute each action sequentially\n        for action in actions:\n            success = self.execute_action(action)\n            if not success:\n                self.node.get_logger().error(f"Action failed: {action}")\n                return False\n\n        return True\n\n    def execute_action(self, action: RobotAction) -> bool:\n        """Execute a single robot action"""\n        self.task_status = TaskStatus.EXECUTING\n\n        if action.action_type == \'navigate\':\n            return self.navigate_to_location(action.location)\n        elif action.action_type == \'pick\':\n            return self.pick_object(action.object_name)\n        elif action.action_type == \'place\':\n            return self.place_object(action.object_name, action.location)\n        else:\n            self.node.get_logger().error(f"Unknown action type: {action.action_type}")\n            return False\n\n    def navigate_to_location(self, location: str) -> bool:\n        """Navigate to a specified location"""\n        if location not in self.known_locations:\n            self.node.get_logger().error(f"Unknown location: {location}")\n            return False\n\n        goal = NavigateToPose.Goal()\n        goal.pose = self.known_locations[location]\n\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal)\n        # Wait for completion\n        rclpy.spin_until_future_complete(self.node, future)\n        return future.result().success\n\n    def pick_object(self, object_name: str) -> bool:\n        """Pick up an object by name"""\n        if object_name not in self.known_objects:\n            self.node.get_logger().error(f"Object not found: {object_name}")\n            return False\n\n        # Move to object location\n        object_pose = self.known_objects[object_name]\n        success = self.move_to_pose(object_pose)\n        if not success:\n            return False\n\n        # Execute pick action\n        return self.execute_pick()\n\n    def object_callback(self, msg: String):\n        """Callback for object detection updates"""\n        # Update known objects based on detection\n        detected_objects = msg.data.split(\',\')\n        for obj in detected_objects:\n            name, x, y, z = obj.split(\':\')\n            self.known_objects[name] = Pose(x=float(x), y=float(y), z=float(z))\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = rclpy.create_node(\'language_to_action_planner\')\n\n    planner = LanguageToActionPlanner(node)\n\n    # Example: Process a command\n    command = "Navigate to the kitchen and pick up the red cup"\n    success = planner.process_command(command)\n\n    node.get_logger().info(f"Command execution {\'succeeded\' if success else \'failed\'}")\n\n    rclpy.spin(node)\n    rclpy.shutdown()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"diagram-placeholders",children:"Diagram Placeholders"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.em,{children:"Diagram showing the pipeline from language input to action execution"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.em,{children:"Architecture for language-driven task planning"})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Language to action planning bridges natural language understanding with robot execution, enabling intuitive human-robot interaction. This requires sophisticated parsing, grounding, and planning techniques to convert human commands into executable robot behaviors."}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a semantic parser for a specific robot domain."}),"\n",(0,a.jsx)(e.li,{children:"Create a task planner that handles multi-step commands."}),"\n",(0,a.jsx)(e.li,{children:"Design a system for resolving ambiguous language commands."}),"\n",(0,a.jsx)(e.li,{children:"Research and compare different approaches to grounded language understanding."}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const o={},i=a.createContext(o);function s(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);
"use strict";(globalThis.webpackChunkai_native_textbook_platform=globalThis.webpackChunkai_native_textbook_platform||[]).push([[607],{1105(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var t=i(4848),r=i(8453);const s={sidebar_position:17,title:"Chapter 17: Voice to Text with Whisper"},o="Chapter 17: Voice to Text with Whisper",a={id:"part2/voice_to_text_with_whisper",title:"Chapter 17: Voice to Text with Whisper",description:"Introduction",source:"@site/docs/part2/17_voice_to_text_with_whisper.md",sourceDirName:"part2",slug:"/part2/voice_to_text_with_whisper",permalink:"/ai-native-textbook-platform/es/docs/part2/voice_to_text_with_whisper",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-humanoid-robots/tree/main/packages/create-docusaurus/templates/shared/docs/part2/17_voice_to_text_with_whisper.md",tags:[],version:"current",sidebarPosition:17,frontMatter:{sidebar_position:17,title:"Chapter 17: Voice to Text with Whisper"},sidebar:"tutorialSidebar",previous:{title:"Chapter 16: What Is Vision-Language-Action (VLA)?",permalink:"/ai-native-textbook-platform/es/docs/part2/what_is_vision_language_action"},next:{title:"Chapter 18: Language to Action Planning",permalink:"/ai-native-textbook-platform/es/docs/part2/language_to_action_planning"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Whisper Model Architecture",id:"whisper-model-architecture",level:3},{value:"Speech Recognition in Robotics",id:"speech-recognition-in-robotics",level:3},{value:"Real-Time Processing",id:"real-time-processing",level:3},{value:"Robotics Integration",id:"robotics-integration",level:3},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Basic Whisper Integration",id:"example-1-basic-whisper-integration",level:3},{value:"Example 2: Streaming Speech Recognition",id:"example-2-streaming-speech-recognition",level:3},{value:"Diagram Placeholders",id:"diagram-placeholders",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-17-voice-to-text-with-whisper",children:"Chapter 17: Voice to Text with Whisper"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI's Whisper model has revolutionized automatic speech recognition (ASR) by providing highly accurate, robust speech-to-text capabilities. This chapter explores the integration of Whisper with robotics systems for voice-controlled robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the Whisper model architecture and capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Learn about speech recognition in robotics applications"}),"\n",(0,t.jsx)(n.li,{children:"Explore real-time speech processing for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Implement Whisper integration with robot control systems"}),"\n",(0,t.jsx)(n.li,{children:"Recognize the benefits of voice interfaces for robotics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"whisper-model-architecture",children:"Whisper Model Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Advanced speech recognition model:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer Architecture"}),": Attention-based neural network"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support"}),": Trained on 98+ languages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Handles various accents, background noise, and speaking styles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Large-Scale Training"}),": Trained on 680,000 hours of multilingual data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-Shot Capabilities"}),": Works without task-specific fine-tuning"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"speech-recognition-in-robotics",children:"Speech Recognition in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Voice interfaces for robot control:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Interaction"}),": Intuitive human-robot communication"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hands-Free Operation"}),": Useful when hands are occupied"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accessibility"}),": Enabling interaction for users with mobility limitations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support"}),": Serving diverse user populations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Awareness"}),": Understanding commands in context"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-time-processing",children:"Real-Time Processing"}),"\n",(0,t.jsx)(n.p,{children:"Challenges in real-time speech recognition:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency Requirements"}),": Low-latency processing for natural interaction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Streaming Audio"}),": Processing continuous audio streams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VAD Integration"}),": Voice Activity Detection to identify speech segments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Buffer Management"}),": Efficient handling of audio data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Constraints"}),": Running on embedded robotics hardware"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"robotics-integration",children:"Robotics Integration"}),"\n",(0,t.jsx)(n.p,{children:"Connecting speech recognition with robot systems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS Integration"}),": Publishing recognized text as ROS messages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Recognition"}),": Converting text to robot commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Managing recognition errors and ambiguities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback Systems"}),": Providing audio/visual feedback to users"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Privacy Considerations"}),": Handling sensitive voice data"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,t.jsx)(n.h3,{id:"example-1-basic-whisper-integration",children:"Example 1: Basic Whisper Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import whisper\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\n\nclass WhisperSpeechToText(Node):\n    def __init__(self):\n        super().__init__('whisper_speech_to_text')\n\n        # Load Whisper model\n        self.model = whisper.load_model(\"base\")\n\n        # Audio parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n\n        # Initialize audio\n        self.audio = pyaudio.PyAudio()\n\n        # Publisher for recognized text\n        self.text_pub = self.create_publisher(String, 'recognized_text', 10)\n\n        # Timer for continuous recognition\n        self.timer = self.create_timer(5.0, self.recognize_audio)\n\n    def recognize_audio(self):\n        # Record audio\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        frames = []\n        for i in range(0, int(self.rate / self.chunk * 5)):  # 5 seconds\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        stream.stop_stream()\n        stream.close()\n\n        # Convert to numpy array and process\n        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)\n        audio_float = audio_data.astype(np.float32) / 32768.0\n\n        # Transcribe with Whisper\n        result = self.model.transcribe(audio_float)\n        text = result[\"text\"]\n\n        # Publish recognized text\n        msg = String()\n        msg.data = text\n        self.text_pub.publish(msg)\n\n        self.get_logger().info(f'Recognized: {text}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperSpeechToText()\n    rclpy.spin(whisper_node)\n    whisper_node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-streaming-speech-recognition",children:"Example 2: Streaming Speech Recognition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\nimport numpy as np\nfrom queue import Queue\nimport threading\n\nclass StreamingWhisper:\n    def __init__(self, model_size="base"):\n        self.model = whisper.load_model(model_size)\n        self.audio_queue = Queue()\n        self.result_queue = Queue()\n        self.is_running = False\n\n    def start_streaming(self):\n        self.is_running = True\n        recognition_thread = threading.Thread(target=self._process_audio)\n        recognition_thread.start()\n\n    def _process_audio(self):\n        while self.is_running:\n            if not self.audio_queue.empty():\n                audio_chunk = self.audio_queue.get()\n\n                # Transcribe the audio chunk\n                result = self.model.transcribe(audio_chunk)\n\n                # Put result in queue for processing\n                self.result_queue.put(result)\n\n    def add_audio_chunk(self, audio_data):\n        self.audio_queue.put(audio_data)\n\n    def get_transcription(self):\n        if not self.result_queue.empty():\n            return self.result_queue.get()\n        return None\n\n# Usage in robot system\nstreaming_whisper = StreamingWhisper()\nstreaming_whisper.start_streaming()\n\n# Continuously add audio chunks and get results\nwhile robot_running:\n    audio_chunk = get_microphone_audio()\n    streaming_whisper.add_audio_chunk(audio_chunk)\n\n    result = streaming_whisper.get_transcription()\n    if result:\n        process_command(result["text"])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"diagram-placeholders",children:"Diagram Placeholders"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Diagram showing the Whisper model architecture"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Pipeline for voice-to-text processing in robotics"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Whisper provides state-of-the-art speech recognition capabilities that can be integrated into robotics systems for natural voice interaction. Understanding its architecture and integration patterns is crucial for developing voice-controlled robots."}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Install and test Whisper on your robotics platform."}),"\n",(0,t.jsx)(n.li,{children:"Implement a voice command recognition system for robot navigation."}),"\n",(0,t.jsx)(n.li,{children:"Compare Whisper with other speech recognition systems for robotics."}),"\n",(0,t.jsx)(n.li,{children:"Research and describe techniques for improving real-time performance."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);
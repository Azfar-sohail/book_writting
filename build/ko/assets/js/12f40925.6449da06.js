"use strict";(globalThis.webpackChunkai_native_textbook_platform=globalThis.webpackChunkai_native_textbook_platform||[]).push([[126],{8453(t,i,n){n.d(i,{R:()=>s,x:()=>r});var o=n(6540);const a={},e=o.createContext(a);function s(t){const i=o.useContext(e);return o.useMemo(function(){return"function"==typeof t?t(i):{...i,...t}},[i,t])}function r(t){let i;return i=t.disableParentContext?"function"==typeof t.components?t.components(a):t.components||a:s(t.components),o.createElement(e.Provider,{value:i},t.children)}},8876(t,i,n){n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>e,metadata:()=>r,toc:()=>l});var o=n(4848),a=n(8453);const e={sidebar_position:0,title:"Part II: Digital Twins, Autonomy & Vision-Language-Action"},s="Part II: Digital Twins, Autonomy & Vision-Language-Action",r={id:"part2/index",title:"Part II: Digital Twins, Autonomy & Vision-Language-Action",description:"This part explores advanced topics in AI for robotics, including simulation environments, perception systems, and autonomous decision making. It covers physics simulation with Gazebo, sensor simulation, Unity for human-robot interaction, NVIDIA Isaac Sim, Isaac ROS navigation, vision-language-action systems, speech recognition, language-to-action planning, autonomous decision making, and concludes with a comprehensive capstone project.",source:"@site/docs/part2/index.md",sourceDirName:"part2",slug:"/part2/",permalink:"/ai-native-textbook-platform/ko/docs/part2/",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-humanoid-robots/tree/main/packages/create-docusaurus/templates/shared/docs/part2/index.md",tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0,title:"Part II: Digital Twins, Autonomy & Vision-Language-Action"},sidebar:"tutorialSidebar",previous:{title:"Chapter 10: URDF & Robot Description",permalink:"/ai-native-textbook-platform/ko/docs/part1/urdf_robot_description"},next:{title:"Chapter 11: Gazebo: Physics & Simulation",permalink:"/ai-native-textbook-platform/ko/docs/part2/gazebo_physics_simulation"}},c={},l=[{value:"Chapters in this Part:",id:"chapters-in-this-part",level:2}];function p(t){const i={a:"a",h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,a.R)(),...t.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.h1,{id:"part-ii-digital-twins-autonomy--vision-language-action",children:"Part II: Digital Twins, Autonomy & Vision-Language-Action"}),"\n",(0,o.jsx)(i.p,{children:"This part explores advanced topics in AI for robotics, including simulation environments, perception systems, and autonomous decision making. It covers physics simulation with Gazebo, sensor simulation, Unity for human-robot interaction, NVIDIA Isaac Sim, Isaac ROS navigation, vision-language-action systems, speech recognition, language-to-action planning, autonomous decision making, and concludes with a comprehensive capstone project."}),"\n",(0,o.jsx)(i.h2,{id:"chapters-in-this-part",children:"Chapters in this Part:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:["Chapter 11: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/gazebo_physics_simulation",children:"Gazebo: Physics & Simulation"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 12: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/sensors_in_simulation",children:"Sensors in Simulation"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 13: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/unity_for_human_robot_interaction",children:"Unity for Human-Robot Interaction"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 14: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/nvidia_isaac_sim",children:"NVIDIA Isaac Sim"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 15: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/isaac_ros_navigation_nav2",children:"Isaac ROS & Navigation (Nav2)"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 16: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/what_is_vision_language_action",children:"What Is Vision-Language-Action (VLA)?"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 17: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/voice_to_text_with_whisper",children:"Voice to Text with Whisper"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 18: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/language_to_action_planning",children:"Language to Action Planning"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 19: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/autonomous_decision_making",children:"Autonomous Decision Making"})]}),"\n",(0,o.jsxs)(i.li,{children:["Chapter 20: ",(0,o.jsx)(i.a,{href:"/ai-native-textbook-platform/ko/docs/part2/capstone_the_autonomous_humanoid",children:"Capstone: The Autonomous Humanoid"})]}),"\n"]}),"\n",(0,o.jsx)(i.p,{children:"After completing this part, readers will have a comprehensive understanding of how to integrate all components into a complete autonomous robotics system."})]})}function h(t={}){const{wrapper:i}={...(0,a.R)(),...t.components};return i?(0,o.jsx)(i,{...t,children:(0,o.jsx)(p,{...t})}):p(t)}}}]);